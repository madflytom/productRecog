<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Counterfeit Detection App</title>
  <style>
    body { font-family: sans-serif; text-align: center; }
    video, canvas { width: 100%; max-width: 400px; border: 1px solid #ccc; }
    #status { margin-top: 10px; font-weight: bold; }
  </style>
</head>
<body>
  <h1>Scan Your Product</h1>
  <video id="video" autoplay playsinline></video>
  <canvas id="canvas" style="display: none;"></canvas>
  <p id="status">Initializing...</p>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.11.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js"></script>
  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const statusEl = document.getElementById('status');

    let objectDetector;

    // Dummy embedding function (replace with actual model-based feature extraction)
    async function extractFeatures(imageTensor) {
      return tf.tidy(() => {
        const resized = tf.image.resizeBilinear(imageTensor, [128, 128]);
        const gray = resized.mean(2);
        const vector = gray.flatten();
        const embedding = vector.slice([0], [128]);
        return embedding.div(255.0);
      });
    }

    // Dummy reference embedding (you'd generate this offline from known good images)
    const referenceEmbedding = tf.randomUniform([128]);

    async function compareEmbeddings(embedding1, embedding2) {
      const diff = tf.sub(embedding1, embedding2);
      const distance = tf.norm(diff);
      return distance.dataSync()[0];
    }

    async function processFrame() {
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      ctx.drawImage(video, 0, 0);
      const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
      const imgTensor = tf.browser.fromPixels(imageData);

      // Check for product shape using MediaPipe
      const mpImage = new vision.ImageData(imageData);
      const detections = await objectDetector.detect(mpImage);
      const hasProduct = detections.detections && detections.detections.length > 0;

      if (!hasProduct) {
        statusEl.textContent = 'üïµÔ∏è Product not detected, please adjust camera.';
      } else {
        const features = await extractFeatures(imgTensor);
        const similarity = await compareEmbeddings(features, referenceEmbedding);

        if (similarity < 3.0) {
          statusEl.textContent = '‚úÖ Product Verified';
        } else {
          statusEl.textContent = '‚ùå Product Mismatch';
        }

        features.dispose();
      }

      imgTensor.dispose();
    }

    async function createObjectDetector() {
      const vision = await FilesetResolver.forVisionTasks(
        'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm'
      );
      objectDetector = await ObjectDetector.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/float16/1/object_detector.task',
          delegate: 'GPU'
        },
        scoreThreshold: 0.5,
        runningMode: 'IMAGE'
      });
    }

    async function startCamera() {
      try {
        await createObjectDetector();
        const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' } });
        video.srcObject = stream;
        video.onloadedmetadata = () => {
          video.play();
          setInterval(processFrame, 2000);
        };
        statusEl.textContent = 'Point your camera at the product...';
      } catch (err) {
        console.error('Error accessing camera:', err);
        statusEl.textContent = 'Camera access denied or MediaPipe error.';
      }
    }

    startCamera();
  </script>
</body>
</html>
