<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Counterfeit Detection App 2</title>
  <style>
    body { font-family: sans-serif; text-align: center; }
    video, canvas { width: 100%; max-width: 400px; border: 1px solid #ccc; }
    #status { margin-top: 10px; font-weight: bold; }
  </style>
</head>
<body>
  <h1>Scan Your Product</h1>
  <video id="video" autoplay playsinline></video>
  <canvas id="canvas" style="display: none;"></canvas>
  <p id="status">Initializing...</p>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.11.0/dist/tf.min.js"></script>
  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const statusEl = document.getElementById('status');

    // Dummy embedding function (replace with actual model-based feature extraction)
    async function extractFeatures(imageTensor) {
      return tf.tidy(() => {
        const resized = tf.image.resizeBilinear(imageTensor, [128, 128]);
        const gray = resized.mean(2);
        const vector = gray.flatten();
        const embedding = vector.slice([0], [128]);
        return embedding.div(255.0);
      });
    }

    // Dummy reference embedding (you'd generate this offline from known good images)
    const referenceEmbedding = tf.randomUniform([128]);

    async function compareEmbeddings(embedding1, embedding2) {
      const diff = tf.sub(embedding1, embedding2);
      const distance = tf.norm(diff);
      return distance.dataSync()[0];
    }

    function detectBoxEdges(imageTensor) {
      return tf.tidy(() => {
        const gray = imageTensor.mean(2).toFloat().div(255);
        const sobel = tf.image.sobelEdges(gray.expandDims(-1));
        const magnitude = tf.sqrt(tf.add(tf.square(sobel.slice([0, 0, 0, 0], [-1, -1, -1, 1])), sobel.slice([0, 0, 0, 1], [-1, -1, -1, 1])));
        const meanMag = magnitude.mean().dataSync()[0];
        return meanMag > 0.1; // crude edge detection threshold
      });
    }

    async function processFrame() {
      if (!video.videoWidth || !video.videoHeight) return;

      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      ctx.drawImage(video, 0, 0);
      const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
      const imgTensor = tf.browser.fromPixels(imageData);

      try {
        const hasBoxEdges = detectBoxEdges(imgTensor);

        if (!hasBoxEdges) {
          statusEl.textContent = 'ðŸ“¦ No boxy shapes detected. Move camera closer or adjust angle.';
        } else {
          const features = await extractFeatures(imgTensor);
          const similarity = await compareEmbeddings(features, referenceEmbedding);

          if (similarity < 3.0) {
            statusEl.textContent = 'âœ… Product Verified. Slowly rotate your phone around the box...';
          } else {
            statusEl.textContent = 'âš ï¸ Shape found, but product mismatch. Try another angle.';
          }

          features.dispose();
        }
      } catch (error) {
        console.error('Detection error:', error);
        statusEl.textContent = `Detection error: ${error.message || error}`;
      }

      imgTensor.dispose();
    }

    async function startCamera() {
      try {
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
          throw new Error('getUserMedia not supported on this browser.');
        }

        const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' } });
        video.srcObject = stream;

        video.onloadedmetadata = () => {
          video.play();
          setInterval(processFrame, 2000);
        };

        statusEl.textContent = 'Point your camera at the product...';
      } catch (err) {
        console.error('Detailed error:', err);
        statusEl.textContent = `Error: ${err.message || err}`;
      }
    }

    startCamera();
  </script>
</body>
</html>
